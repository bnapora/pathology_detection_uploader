{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "subscription_id = os.getenv(\"<Enter Azure Subscription #>\", default=\"<Enter Azure Subscription #>\")\n",
        "resource_group = os.getenv(\"gestaltml\", default=\"gestaltml\")\n",
        "workspace_name = os.getenv(\"fastai2\", default=\"fastai2\")\n",
        "workspace_region = os.getenv(\"eastus\", default=\"eastus\")\n",
        "\n",
        "score_script = 'score_local.py'\n",
        "model_name = 'breastcancerfastai_local03'\n",
        "service_name = 'breastcancerlocal03'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "fastai2\ngestaltml\neastus\n40bffbcc-578f-4e44-bd6d-972552eb6513\n"
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.get(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group)\n",
        "\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model breastcancerfastai_local03\n"
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path=\"export.pkl\",\n",
        "                       model_name=model_name,\n",
        "                       tags={'area': \"breast\", 'type': \"transfer-learning\", 'classes':'Positive,Negative'},\n",
        "                       description=\"Breast Cancer Diag-embeded X,Y functions:092220\",\n",
        "                       workspace=ws)"
      ]
    },
    {
      "source": [
        "## Open Existing Model<br>\n",
        "Use if model already installed in Azure ML Workspace"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: breastcancerfastai_local03\nVersion: 1\n"
        }
      ],
      "source": [
        "# from azureml.core import Model\n",
        "# from azureml.core.resource_configuration import ResourceConfiguration\n",
        "\n",
        "# model = Model(ws, model_name)\n",
        "\n",
        "# print('Name:', model.name)\n",
        "# print('Version:', model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Manage Docker dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "source_directory = \"source_directory\"\n",
        "\n",
        "os.makedirs(source_directory, exist_ok=True)\n",
        "os.makedirs(os.path.join(source_directory, \"x/y\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(source_directory, \"env\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(source_directory, \"dockerstep\"), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display scoring file (var score_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "import os, base64\nimport glob\nimport re\nimport torch\nimport json\nfrom io import BytesIO\nfrom azureml.core.model import Model\nfrom azureml.core import Workspace\nimport fastai \nfrom fastai.vision.all import *\nfrom fastai.callback.all import *\nimport urllib.request\n\n\n\ndef init():\n    global learner\n    pathsrc=Path()\n\n    model_path=os.getenv('AZUREML_MODEL_DIR')  \n    \n    filename=\"export.pkl\"\n    classes = ['Positive','Negative']\n    model_path_file = os.path.join(os.environ['AZUREML_MODEL_DIR'], filename)\n    model_path_hc = 'azureml-models/breastcancerfastai/1'\n    print('AZUREML_MODEL_DIR-Model_Path: ' + model_path)\n    print('Model_Path_File: ' + model_path_file)\n    print('FastAI Version: ' + fastai.__version__)\n    print ('Path(): ' + str(pathsrc))\n\n    for filename in os.listdir(pathsrc):\n        print(filename)\n\n    for dirname, dirnames, filenames in os.walk(pathsrc):\n        # print path to all subdirectories first.\n        for subdirname in dirnames:\n            print(os.path.join(dirname, subdirname))\n\n        # print path to all filenames.\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n    learner = load_learner(model_path_file)\n    classes = learner.dls.vocab\n    print(learner.dls.vocab)\n\ndef encode(img):\n    img = Image.open(BytesIO(img))\n    img = img.resize((500,500))\n    buff = BytesIO()\n    img.save(buff, format='jpeg')\n    return base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n\ndef preprocess(image):\n    test = image\n    data = str(base64.b64encode(test), encoding='utf-8')  \n    input_data = json.dumps({'data': data})\n    return input_data\n\ndef run(raw_data):\n    base64_string = json.loads(raw_data)['data']\n    base64_bytes = base64.b64decode(base64_string)\n\n    # img = Image.open(BytesIO(base64_bytes))\n    img = PILImage.create(BytesIO(base64_bytes))\n\n    pred_class,pred_idx,outputs = learner.predict(img)\n\n    formatted_outputs = [\"{:.1f}%\".format(value) for value in [x * 100 for x in torch.nn.functional.softmax(outputs, dim=0)]]\n    pred_probs = sorted(\n            zip(learner.dls.vocab, map(str, formatted_outputs)),\n            key=lambda p: p[1],\n            reverse=True\n        )\n\n    #img_data = preprocess(img)\n\n    result = {'class':pred_class, \"probs\":pred_probs}\n    # result = {\"class\":pred_class, \"probs\":pred_probs, \"image\":img_data}\n    # return json.dumps({'class':str(result[0]), 'probs':result[2].data[1].item()})\n    return json.dumps(result)\n"
        }
      ],
      "source": [
        "with open(score_script) as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Sample for writing test data to Docker build directory\n",
        "# %%writefile source_directory/extradata.json\n",
        "# {\n",
        "#     \"people\": [\n",
        "#         {\n",
        "#             \"website\": \"microsoft.com\", \n",
        "#             \"from\": \"Seattle\", \n",
        "#             \"name\": \"Mrudula\"\n",
        "#         }\n",
        "#     ]\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Inference Configuration\n",
        "\n",
        " - file_path: input parameter to Environment constructor. Manages conda and python package dependencies.\n",
        " - env.docker.base_dockerfile: any extra steps you want to inject into docker file\n",
        " - source_directory: holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
        " - entry_script: contains logic specific to initializing your model and running predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create environment\n",
        "\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "entryscript_fullpath = \"x/y/\" + str(score_script)\n",
        "\n",
        "myenv = Environment(model_name)\n",
        "myenv.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
        "    'azureml-defaults~=1.13.0', 'fastcore==1.0.0', 'fastprogress==1.0.0', 'fastscript==1.0.0', 'Pillow==5.4.1', 'requests', 'torch==1.6.0', 'torchvision>=0.5.0', 'fastai==2.0.6', 'ipython'\n",
        "])\n",
        "# explicitly set base_image to None when setting base_dockerfile\n",
        "myenv.docker.base_image = None\n",
        "myenv.docker.base_dockerfile = \"FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\\nRUN echo \\\"this is test\\\"\"\n",
        "myenv.inferencing_stack_version = \"latest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Create inference config\n",
        "inference_config = InferenceConfig(source_directory=source_directory,                                 entry_script=entryscript_fullpath,\n",
        "    environment=myenv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy Model as a Local Docker Web Service\n",
        "\n",
        "*Make sure you have Docker installed and running.*\n",
        "\n",
        "Note that the service creation can take few minutes.\n",
        "\n",
        "NOTE:\n",
        "\n",
        "The Docker image runs as a Linux container. If you are running Docker for Windows, you need to ensure the Linux Engine is running:\n",
        "\n",
        "    # PowerShell command to switch to Linux engine\n",
        "    & 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchLinuxEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading model breastcancerfastai_local03:1 to C:\\Users\\BRIANN~1\\AppData\\Local\\Temp\\azureml_vqc172sx\\breastcancerfastai_local03\\1\nGenerating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry 84659b36a99e4a16a8757ae52b4288dc.azurecr.io\nLogging into Docker registry 84659b36a99e4a16a8757ae52b4288dc.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1/5 : FROM 84659b36a99e4a16a8757ae52b4288dc.azurecr.io/azureml/azureml_e5a6bc09d9ffd8bce12ea2b9ad46d390\n ---> 7a313ce8cfc0\nStep 2/5 : COPY azureml-app /var/azureml-app\n ---> fc3ddb08d5a4\nStep 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjQwYmZmYmNjLTU3OGYtNGU0NC1iZDZkLTk3MjU1MmViNjUxMyIsInJlc291cmNlR3JvdXBOYW1lIjoiZ2VzdGFsdG1sIiwiYWNjb3VudE5hbWUiOiJmYXN0YWkyIiwid29ya3NwYWNlSWQiOiI4NDY1OWIzNi1hOTllLTRhMTYtYTg3NS03YWU1MmI0Mjg4ZGMifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode > /var/azureml-app/model_config_map.json\n ---> Running in 77f1e98ee2d7\n ---> defd71d656a8\nStep 4/5 : RUN mv '/var/azureml-app/tmp5kei6ymo.py' /var/azureml-app/main.py\n ---> Running in f2089cbc0159\n ---> a26c70b286f7\nStep 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n ---> Running in d626c9b72f5a\n ---> 70918e535f40\nSuccessfully built 70918e535f40\nSuccessfully tagged breastcancerlocal03:latest\nStarting Docker container...\nDocker container running.\nChecking container health...\nLocal webservice is running at http://localhost:6789\n"
        }
      ],
      "source": [
        "from azureml.core.webservice import LocalWebservice\n",
        "\n",
        "# This is optional, if not provided Docker will choose a random unused port.\n",
        "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
        "\n",
        "local_service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
        "\n",
        "local_service.wait_for_deployment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Local service port: 6789\n"
        }
      ],
      "source": [
        "print('Local service port: {}'.format(local_service.port))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Status and Get Container Logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2020-09-22T20:37:37,674009100+00:00 - rsyslog/run \n2020-09-22T20:37:37,673979200+00:00 - iot-server/run \n2020-09-22T20:37:37,673965600+00:00 - gunicorn/run \n2020-09-22T20:37:37,676363400+00:00 - nginx/run \n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2020-09-22T20:37:37,912574900+00:00 - iot-server/finish 1 0\n2020-09-22T20:37:37,914003100+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http://127.0.0.1:31311 (10)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 45\nSPARK_HOME not set. Skipping PySpark Initialization.\nGenerating new fontManager, this may take some time...\nInitializing logger\n2020-09-22 20:37:43,542 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-09-22 20:37:43,543 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-09-22 20:37:43,543 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-09-22 20:37:43,543 | root | INFO | Invoking user's init function\nInvoking user's init function\nAZUREML_MODEL_DIR-Model_Path: azureml-models/breastcancerfastai_local03/1\nModel_Path_File: azureml-models/breastcancerfastai_local03/1/export.pkl\nModel_Path_File_HC: ./source_directory/azureml-models/breastcancerfastai/1/export_breast_092220.pkl\nFastAI Version: 2.0.6\nPath(): .\nazureml-models\nsource_directory\n__pycache__\nmain.py\nmodel_config_map.json\n./azureml-models\n./source_directory\n./__pycache__\n./main.py\n./model_config_map.json\n./azureml-models/breastcancerfastai_local03\n./azureml-models/breastcancerfastai_local03/1\n./azureml-models/breastcancerfastai_local03/1/export.pkl\n./source_directory/azureml-models\n./source_directory/dockerstep\n./source_directory/env\n./source_directory/x\n./source_directory/azureml-models/breastcancerfastai\n./source_directory/azureml-models/breastcancerfastai/1\n./source_directory/azureml-models/breastcancerfastai/1/export_breast_092220.pkl\n./source_directory/x/y\n./source_directory/x/y/__pycache__\n./source_directory/x/y/score_local.py\n./source_directory/x/y/__pycache__/score_local.cpython-36.pyc\n./__pycache__/main.cpython-36.pyc\n(#2) ['Negative','Positive']\n2020-09-22 20:37:45,312 | root | INFO | Users's init has completed successfully\nUsers's init has completed successfully\n2020-09-22 20:37:45,315 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\nSkipping middleware: dbg_model_info as it's not enabled.\n2020-09-22 20:37:45,315 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\nSkipping middleware: dbg_resource_usage as it's not enabled.\n2020-09-22 20:37:45,316 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\nScoring timeout setting is not found. Use default timeout: 3600000 ms\n\n"
        }
      ],
      "source": [
        "print(local_service.get_logs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Web Service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the web service with some input data to get a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test,Positive\n"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "sample_input = 'Testing Data'\n",
        "\n",
        "print(local_service.run(sample_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reload Service\n",
        "\n",
        "You can update your score.py file and then call `reload()` to quickly restart the service. This will only reload your execution script and dependency files, it will not rebuild the underlying Docker image. As a result, `reload()` is fast, but if you do need to rebuild the image -- to add a new Conda or pip package, for instance -- you will have to call `update()`, instead (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%writefile source_directory/x/y/score_template.py\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from inference_schema.schema_decorators import input_schema, output_schema\n",
        "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_local.pkl')\n",
        "    # Deserialize the model file back into a sklearn model.\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    global name, from_location\n",
        "    # Note here, the entire source directory from inference config gets added into image.\n",
        "    # Below is an example of how you can use any extra files in image.\n",
        "    with open('source_directory/extradata.json') as json_file:  \n",
        "        data = json.load(json_file)\n",
        "        name = data[\"people\"][0][\"name\"]\n",
        "        from_location = data[\"people\"][0][\"from\"]\n",
        "\n",
        "input_sample = np.array([[10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0]])\n",
        "output_sample = np.array([3726.995])\n",
        "\n",
        "@input_schema('data', NumpyParameterType(input_sample))\n",
        "@output_schema(NumpyParameterType(output_sample))\n",
        "def run(data):\n",
        "    try:\n",
        "        result = model.predict(data)\n",
        "        # You can return any JSON-serializable object.\n",
        "        return \"Hello \" + name + \" from \" + from_location + \" here is your result = \" + str(result)\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        return error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Container has been successfully cleaned up.\nStarting Docker container...\nDocker container running.\n--------------------------------------------------------------\n"
        }
      ],
      "source": [
        "local_service.reload()\n",
        "print(\"--------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2020-09-23T05:58:40,990570800+00:00 - gunicorn/run \n2020-09-23T05:58:40,990989000+00:00 - iot-server/run \n2020-09-23T05:58:40,992153200+00:00 - rsyslog/run \n2020-09-23T05:58:40,995234300+00:00 - nginx/run \n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n/usr/sbin/nginx: /azureml-envs/azureml_68b223042de9a84a2da2ad7149ac35f9/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2020-09-23T05:58:41,111136600+00:00 - iot-server/finish 1 0\n2020-09-23T05:58:41,112585500+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http://127.0.0.1:31311 (11)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 45\nSPARK_HOME not set. Skipping PySpark Initialization.\nGenerating new fontManager, this may take some time...\nInitializing logger\n2020-09-23 05:58:44,102 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-09-23 05:58:44,102 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-09-23 05:58:44,102 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-09-23 05:58:44,102 | root | INFO | Invoking user's init function\nInvoking user's init function\nAZUREML_MODEL_DIR-Model_Path: azureml-models/breastcancerfastai_local03/1\nModel_Path_File: azureml-models/breastcancerfastai_local03/1/export.pkl\nModel_Path_File_HC: ./source_directory/azureml-models/breastcancerfastai/1/export_breast_092220.pkl\nFastAI Version: 2.0.6\n2020-09-23 05:58:44,103 | root | ERROR | User's init function failed\nUser's init function failed\n2020-09-23 05:58:44,103 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File \"/var/azureml-server/aml_blueprint.py\", line 163, in register\n    main.init()\n  File \"/var/azureml-app/main.py\", line 35, in init\n    driver_module.init()\n  File \"/var/azureml-app/source_directory/x/y/score_local.py\", line 33, in init\n    print('Scoring URI: ' + scoring_uri)\nNameError: name 'scoring_uri' is not defined\n\nEncountered Exception Traceback (most recent call last):\n  File \"/var/azureml-server/aml_blueprint.py\", line 163, in register\n    main.init()\n  File \"/var/azureml-app/main.py\", line 35, in init\n    driver_module.init()\n  File \"/var/azureml-app/source_directory/x/y/score_local.py\", line 33, in init\n    print('Scoring URI: ' + scoring_uri)\nNameError: name 'scoring_uri' is not defined\n\nWorker exiting (pid: 45)\nShutting down: Master\nReason: Worker failed to boot.\n2020-09-23T05:58:44,496530700+00:00 - gunicorn/finish 3 0\n2020-09-23T05:58:44,498054700+00:00 - Exit code 3 is not normal. Killing image.\n\n"
        }
      ],
      "source": [
        "print(local_service.get_logs())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After calling reload(), run() will return the updated message.\n",
        "local_service.run('Testing')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update Service\n",
        "\n",
        "If you want to change your model(s), Conda dependencies, or deployment configuration, call `update()` to rebuild the Docker image.\n",
        "\n",
        "```python\n",
        "\n",
        "local_service.update(models=[SomeOtherModelObject],\n",
        "                     deployment_config=local_config,\n",
        "                     inference_config=inference_config)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "local_service.update(inference_config=inference_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Container (name:distracted_hertz, id:8ad28d69da5e707bf0e44b12db78846c6aeb8f125de6fbebbb986410e78112e4) cannot be killed.\nContainer has been successfully cleaned up.\n"
        }
      ],
      "source": [
        "local_service.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "http://localhost:6789/score\n"
        }
      ],
      "source": [
        "print(local_service.scoring_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "keriehm"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('machinelearningnotebooks': conda)",
      "language": "python",
      "name": "python_defaultSpec_1600877834151"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}